# ============================================================
# 词表扩充配置文件
# ============================================================
# 用途：配置词表扩充的完整工作流
# 工作流程：
#   1. 提取基座 tokenizer 配置 → baseline
#   2. 准备训练语料 → data
#   3. 训练辅助 tokenizer → aux_tokenizer
#   4. 筛选新增 token → selection
#   5. 合并到基座 → extended_tokenizer
#   6. 扩展模型 embedding → extended_model
#   7. 评估效果 → tokenizer/evaluate/
# ============================================================

# ------------------------------------------------------------
# 基座配置
# ------------------------------------------------------------

# 基座 tokenizer 名称或路径
# 说明：作为扩充的基础，通常是 HF Hub 上的模型
# 示例：
#   - bert-base-chinese（BERT 中文）
#   - Qwen/Qwen2.5-0.5B（千问）
#   - meta-llama/Llama-2-7b-hf（LLaMA2）
base_tokenizer: bert-base-chinese

# 基座模型名称或路径
# 说明：用于扩展 embedding 层，通常与 base_tokenizer 相同
base_model: bert-base-chinese

# ------------------------------------------------------------
# 数据配置
# ------------------------------------------------------------

data:
  # HuggingFace 数据集名称
  # 说明：用于获取训练语料，支持 HF Hub 上的所有数据集
  # 示例：shibing624/medical, wikitext, c4
  dataset_name: shibing624/medical
  
  # 数据子目录列表
  # 说明：某些数据集有多个子目录（如 pretrain/finetune/reward）
  # 脚本会分别下载每个子目录的数据
  data_subdirs:
  - pretrain    # 预训练数据
  - finetune    # 微调数据
  - reward      # 奖励模型数据
  
  # 数据集版本/分支
  # 说明：指定 HF Hub 上的特定版本或分支
  # 示例：refs/convert/parquet（Parquet 格式分支）
  revision: refs/convert/parquet
  
  # 每个拆分的采样限制
  # 说明：0 表示全量下载，>0 表示每个拆分采样指定数量
  # 建议：测试时用 1000，正式训练用 0（全量）
  sample_limit: 1000
  
  # 强制重新下载
  # 说明：忽略本地缓存，从 HF Hub 重新下载
  # 建议：首次下载或数据更新时启用
  force_download: true
  
  # 原始数据输出目录
  # 说明：fetch_dataset.py 下载的原始数据存放位置
  output_dir: data_process/fetch_data
  
  # 处理后数据输出目录
  # 说明：process_data.py 清洗后的数据存放位置
  final_dir: data_process/final_data
  
  # Tokenizer 训练语料目录
  # 说明：专门用于训练 tokenizer 的语料存放位置
  # 通常是从 final_dir 中提取的纯文本
  tokenizer_corpus_dir: tokenizer/data/tokenizer_data
  
  # 语料准备日志路径
  # 说明：记录语料清洗、去重等统计信息
  tokenizer_corpus_log: tokenizer/data/tokenizer_data/data_prep.log

# ------------------------------------------------------------
# 基线配置
# ------------------------------------------------------------

baseline:
  # 基座 tokenizer 配置快照路径
  # 说明：保存基座 tokenizer 的配置（vocab_size, special_tokens 等）
  # 用途：用于对比和回滚
  config_path: tokenizer/logs/baseline_tokenizer_config.json

# ------------------------------------------------------------
# 辅助 Tokenizer 配置
# ------------------------------------------------------------

aux_tokenizer:
  # 辅助 tokenizer 输出目录
  # 说明：存放训练好的辅助 tokenizer 文件
  output_dir: tokenizer/aux_tokenizer
  
  # SentencePiece 模型文件路径
  # 说明：训练后的 .model 文件，包含新增的子词
  sp_model: tokenizer/aux_tokenizer/chinese_spm_20000.model
  
  # 词表大小
  # 说明：辅助 tokenizer 的 vocab_size（仅新增部分）
  # 取值范围：10000-30000
  # 建议：
  #   - 小规模扩充：10000-15000
  #   - 中规模扩充：15000-20000
  #   - 大规模扩充：20000-30000
  vocab_size: 20000
  
  # 模型类型
  # 可选值：bpe / unigram
  # 说明：
  #   - bpe：Byte Pair Encoding，直观易理解
  #   - unigram：Unigram Language Model，更稳定
  # 推荐：bpe（与 LLaMA/Qwen 等主流模型一致）
  model_type: bpe
  
  # 字符覆盖率
  # 说明：控制长尾字符的保留比例
  # 取值范围：0.9990-0.9999
  # 推荐：
  #   - 中文/多语：0.9995
  #   - 纯英文：0.999
  character_coverage: 0.9995
  
  # 是否拆分数字
  # 说明：将数字按位拆分（如 123 → 1, 2, 3）
  # 推荐：true（与 LLaMA 等模型一致）
  split_digits: true
  
  # 是否使用 byte fallback
  # 说明：未知字符用 byte 表示，避免 UNK
  # 推荐：true（保证所有字符都能编码）
  byte_fallback: true
  
  # 最大句子长度
  # 说明：超过此长度的句子会被截断
  # 建议：24000（足够长，避免截断长文档）
  max_sentence_length: 24000
  
  # 训练语料路径（glob 模式）
  # 说明：用于训练辅助 tokenizer 的文本文件
  # 格式：每行一个句子
  input_glob: tokenizer/data/tokenizer_data/tokenizer_clean.txt
  
  # 训练日志路径
  # 说明：记录训练命令、超参、输入哈希等信息
  log_path: tokenizer/aux_tokenizer/train_spm.log

# ------------------------------------------------------------
# Token 筛选配置
# ------------------------------------------------------------

selection:
  # 输入数据路径（glob 模式）
  # 说明：用于对比分词效果的数据文件
  # 示例：data_process/final_data/pretrain/*train*.jsonl
  input_glob: data_process/final_data/pretrain/*train*.jsonl
  
  # 最大采样数量
  # 说明：从输入数据中采样的最大样本数
  # 建议：10万-50万（平衡速度和准确性）
  max_samples: 100000
  
  # 最小频率阈值
  # 说明：只保留出现次数 >= min_freq 的 token
  # 建议：5-10（过滤低频噪声）
  min_freq: 5
  
  # 基座最大拆分数
  # 说明：在基座 tokenizer 中被拆成 > max_base_pieces 个 token 的才考虑
  # 建议：3（即基座中被拆成 4+ 个 token 的词）
  max_base_pieces: 3
  
  # 新增 token 列表输出路径
  # 说明：筛选后的候选 token 列表
  output: tokenizer/aux_tokenizer/new_tokens_list.txt
  
  # 基线配置路径
  # 说明：用于对比的基座 tokenizer 配置
  baseline_config: tokenizer/logs/baseline_tokenizer_config.json
  
  # 辅助 tokenizer 目录
  # 说明：包含辅助 tokenizer 文件的目录
  aux_tokenizer: tokenizer/aux_tokenizer
  
  # 辅助 SentencePiece 模型路径
  # 说明：用于对比分词的辅助 tokenizer 模型
  aux_sp_model: tokenizer/aux_tokenizer/chinese_spm_20000.model

# ------------------------------------------------------------
# 数据处理配置
# ------------------------------------------------------------

process:
  # 处理后数据输出目录
  # 说明：清洗、去重后的数据存放位置
  output_dir: data_process/final_data
  
  # 最小字符数
  # 说明：过滤掉字符数 < min_chars 的样本
  # 建议：5-10（过滤过短文本）
  min_chars: 5
  
  # 最大字符数
  # 说明：过滤掉字符数 > max_chars 的样本
  # 建议：2000-5000（根据任务调整）
  max_chars: 2000
  
  # 最大对话轮数
  # 说明：对于多轮对话数据，限制轮数
  # 取值：
  #   - 1：单轮对话
  #   - >1：多轮对话
  #   - <=0：不限制
  max_turns: 1
  
  # 是否精确去重
  # 说明：使用哈希去除完全相同的样本
  # 推荐：true
  dedup: true
  
  # 是否相似去重
  # 说明：使用 Jaccard 相似度去除近似重复样本
  # 推荐：true（但会增加处理时间）
  near_dedup: true
  
  # Jaccard 相似度阈值
  # 说明：相似度 > threshold 的样本被视为重复
  # 取值范围：0.8-0.95
  # 建议：0.9（平衡去重效果和误删）
  jaccard_threshold: 0.9
  
  # 情感过滤
  # 可选值：positive / negative / none
  # 说明：
  #   - positive：只保留正面情感
  #   - negative：只保留负面情感
  #   - none：不过滤
  # 注意：需要下载情感分类模型
  sentiment_filter: negative
  
  # 情感分类模型
  # 说明：HuggingFace 上的情感分类模型
  # 示例：uer/roberta-base-finetuned-jd-binary-chinese
  sentiment_model: uer/roberta-base-finetuned-jd-binary-chinese
  
  # 是否输出 Parquet 格式
  # 说明：除了 JSONL，额外输出 Parquet 格式（更高效）
  # 推荐：true
  to_parquet: true

# ------------------------------------------------------------
# 输出路径配置
# ------------------------------------------------------------

# 扩展后的 tokenizer 输出路径
# 说明：合并后的 tokenizer 存放位置
# 用途：用于后续的 CPT/SFT 训练
extended_tokenizer: tokenizer/merged_tokenizer

# 扩展后的模型输出路径
# 说明：扩展 embedding 后的模型存放位置
# 用途：用于后续的 CPT/SFT 训练
extended_model: tokenizer/merged_model
