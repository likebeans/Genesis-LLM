#!/usr/bin/env python3
"""
HuggingFace æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼å¹¶é‡åŒ–

å®Œæ•´æµç¨‹ï¼š
1. åŠ è½½ HuggingFace æ¨¡å‹
2. è½¬æ¢ä¸º GGUF æ ¼å¼ï¼ˆFP16ï¼‰
3. å¯é€‰ï¼šè¿›è¡Œé‡åŒ–ï¼ˆQ4_K_M ç­‰ï¼‰

ä¾èµ–ï¼š
- pip install llama-cpp-python
- æˆ–ç¼–è¯‘ llama.cpp å¹¶å®‰è£…å…¶ Python ä¾èµ–

ç”¨æ³•ï¼š
    # ä»…è½¬æ¢
    python convert_to_gguf.py --model_path /path/to/hf_model --output_path model.gguf

    # è½¬æ¢å¹¶é‡åŒ–
    python convert_to_gguf.py --model_path /path/to/hf_model --output_path model-q4km.gguf --quantize q4_k_m

    # ä½¿ç”¨æœ¬åœ° llama.cpp
    python convert_to_gguf.py --model_path /path/to/hf_model --output_path model.gguf --llama_cpp_path /path/to/llama.cpp
"""

from __future__ import annotations

import argparse
import subprocess
import sys
from pathlib import Path


# æ”¯æŒçš„é‡åŒ–ç±»å‹
QUANTIZE_TYPES = [
    "q2_k", "q3_k_s", "q3_k_m", "q3_k_l",
    "q4_0", "q4_1", "q4_k_s", "q4_k_m",
    "q5_0", "q5_1", "q5_k_s", "q5_k_m",
    "q6_k", "q8_0",
    "f16", "f32",
]


def find_llama_cpp_path(custom_path: str | None = None) -> Path | None:
    """æŸ¥æ‰¾ llama.cpp è·¯å¾„"""
    if custom_path:
        path = Path(custom_path)
        if path.exists():
            return path
        print(f"è­¦å‘Šï¼šæŒ‡å®šçš„ llama.cpp è·¯å¾„ä¸å­˜åœ¨: {custom_path}")

    # å¸¸è§è·¯å¾„
    common_paths = [
        Path.home() / "llama.cpp",
        Path("/opt/llama.cpp"),
        Path("./llama.cpp"),
        Path("../llama.cpp"),
    ]

    for p in common_paths:
        if p.exists() and (p / "convert_hf_to_gguf.py").exists():
            return p

    return None


def convert_hf_to_gguf(
    model_path: str,
    output_path: str,
    llama_cpp_path: Path | None = None,
) -> bool:
    """å°† HuggingFace æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼"""

    print(f"ğŸ“¦ å¼€å§‹è½¬æ¢æ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")

    if llama_cpp_path:
        # ä½¿ç”¨ llama.cpp çš„è½¬æ¢è„šæœ¬
        convert_script = llama_cpp_path / "convert_hf_to_gguf.py"
        if not convert_script.exists():
            # å°è¯•æ—§ç‰ˆè„šæœ¬å
            convert_script = llama_cpp_path / "convert.py"

        if not convert_script.exists():
            print(f"âŒ æ‰¾ä¸åˆ°è½¬æ¢è„šæœ¬: {convert_script}")
            return False

        cmd = [
            sys.executable,
            str(convert_script),
            model_path,
            "--outfile", output_path,
            "--outtype", "f16",
        ]
    else:
        # å°è¯•ä½¿ç”¨ transformers + llama-cpp-python æ–¹å¼
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer

            print("ä½¿ç”¨ transformers åŠ è½½æ¨¡å‹...")
            print("âš ï¸  æ¨èä½¿ç”¨ llama.cpp çš„è½¬æ¢è„šæœ¬ä»¥è·å¾—æœ€ä½³å…¼å®¹æ€§")

            # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦ llama.cpp çš„è½¬æ¢è„šæœ¬
            print("âŒ è¯·æŒ‡å®š --llama_cpp_path å‚æ•°æˆ–å®‰è£… llama.cpp")
            print("   git clone https://github.com/ggerganov/llama.cpp")
            print("   pip install -r llama.cpp/requirements.txt")
            return False

        except ImportError as e:
            print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            return False

    print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(result.stdout)
        if result.stderr:
            print(result.stderr)
        print("âœ… è½¬æ¢å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        if e.stdout:
            print(e.stdout)
        if e.stderr:
            print(e.stderr)
        return False


def quantize_gguf(
    input_path: str,
    output_path: str,
    quantize_type: str,
    llama_cpp_path: Path,
) -> bool:
    """å¯¹ GGUF æ¨¡å‹è¿›è¡Œé‡åŒ–"""

    print(f"ğŸ”§ å¼€å§‹é‡åŒ–: {quantize_type}")

    # æŸ¥æ‰¾é‡åŒ–å·¥å…·
    quantize_bin = llama_cpp_path / "quantize"
    if not quantize_bin.exists():
        quantize_bin = llama_cpp_path / "build" / "bin" / "quantize"

    if not quantize_bin.exists():
        print(f"âŒ æ‰¾ä¸åˆ°é‡åŒ–å·¥å…·: {quantize_bin}")
        print("   è¯·å…ˆç¼–è¯‘ llama.cpp: cd llama.cpp && make")
        return False

    cmd = [str(quantize_bin), input_path, output_path, quantize_type.upper()]

    print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(result.stdout)
        print("âœ… é‡åŒ–å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ é‡åŒ–å¤±è´¥: {e}")
        if e.stdout:
            print(e.stdout)
        if e.stderr:
            print(e.stderr)
        return False


def main():
    parser = argparse.ArgumentParser(
        description="HuggingFace æ¨¡å‹è½¬ GGUF å¹¶é‡åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»…è½¬æ¢ä¸º FP16 GGUF
  python convert_to_gguf.py --model_path ./my_model --output_path model.gguf

  # è½¬æ¢å¹¶é‡åŒ–ä¸º Q4_K_M
  python convert_to_gguf.py --model_path ./my_model --output_path model-q4km.gguf --quantize q4_k_m

é‡åŒ–ç±»å‹è¯´æ˜:
  q8_0    - 8-bit é‡åŒ–ï¼Œç²¾åº¦æœ€é«˜
  q5_k_m  - 5-bit é‡åŒ–ï¼Œå¹³è¡¡ç²¾åº¦ä¸å¤§å°
  q4_k_m  - 4-bit é‡åŒ–ï¼Œæ¨èé€‰æ‹©
  q4_0    - 4-bit é‡åŒ–ï¼Œå…¼å®¹æ€§å¥½
  q3_k_m  - 3-bit é‡åŒ–ï¼Œæé™å‹ç¼©
        """
    )

    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="HuggingFace æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HF hub åç§°ï¼‰"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="è¾“å‡º GGUF æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--quantize",
        type=str,
        choices=QUANTIZE_TYPES,
        default=None,
        help="é‡åŒ–ç±»å‹ï¼ˆä¸æŒ‡å®šåˆ™è¾“å‡º FP16ï¼‰"
    )
    parser.add_argument(
        "--llama_cpp_path",
        type=str,
        default=None,
        help="llama.cpp ç›®å½•è·¯å¾„"
    )

    args = parser.parse_args()

    # æŸ¥æ‰¾ llama.cpp
    llama_cpp_path = find_llama_cpp_path(args.llama_cpp_path)

    if not llama_cpp_path:
        print("âŒ æ‰¾ä¸åˆ° llama.cppï¼Œè¯·é€šè¿‡ --llama_cpp_path æŒ‡å®š")
        print("   æˆ–å…‹éš†åˆ°å¸¸è§ä½ç½®:")
        print("   git clone https://github.com/ggerganov/llama.cpp ~/llama.cpp")
        sys.exit(1)

    print(f"ğŸ“‚ ä½¿ç”¨ llama.cpp: {llama_cpp_path}")

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    output_path = Path(args.output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if args.quantize:
        # éœ€è¦å…ˆè½¬æ¢ä¸º FP16ï¼Œå†é‡åŒ–
        fp16_path = output_path.with_suffix(".fp16.gguf")

        # Step 1: è½¬æ¢ä¸º FP16 GGUF
        if not convert_hf_to_gguf(args.model_path, str(fp16_path), llama_cpp_path):
            sys.exit(1)

        # Step 2: é‡åŒ–
        if not quantize_gguf(str(fp16_path), str(output_path), args.quantize, llama_cpp_path):
            sys.exit(1)

        # å¯é€‰ï¼šåˆ é™¤ä¸­é—´ FP16 æ–‡ä»¶
        # fp16_path.unlink()
        print(f"ğŸ’¡ FP16 ä¸­é—´æ–‡ä»¶ä¿ç•™åœ¨: {fp16_path}")

    else:
        # ä»…è½¬æ¢ä¸º FP16
        if not convert_hf_to_gguf(args.model_path, str(output_path), llama_cpp_path):
            sys.exit(1)

    print(f"\nğŸ‰ å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


if __name__ == "__main__":
    main()
