#!/usr/bin/env python3
"""
GPTQ é‡åŒ–è„šæœ¬

GPTQ (Generative Pre-trained Transformer Quantization) æ˜¯ä¸€ç§åŸºäº Hessian çŸ©é˜µçš„
æƒé‡é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡é€å±‚é‡åŒ–å’Œè¯¯å·®è¡¥å¿å®ç°é«˜å‹ç¼©æ¯”ã€‚

ä¾èµ–ï¼š
    pip install auto-gptq optimum transformers

ç”¨æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•
    python gptq_quantize.py --model_path /path/to/model --output_path /path/to/output

    # è‡ªå®šä¹‰å‚æ•°
    python gptq_quantize.py \
        --model_path Qwen/Qwen2.5-7B-Instruct \
        --output_path ./qwen2.5-7b-gptq \
        --bits 4 \
        --group_size 128 \
        --desc_act true
"""

from __future__ import annotations

import argparse
from pathlib import Path


def quantize_gptq(
    model_path: str,
    output_path: str,
    bits: int = 4,
    group_size: int = 128,
    desc_act: bool = True,
    damp_percent: float = 0.1,
    calib_dataset: str = "wikitext2",
    calib_samples: int = 1024,
    calib_seq_len: int = 512,
    use_exllama: bool = True,
):
    """æ‰§è¡Œ GPTQ é‡åŒ–"""

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("   è¯·å®‰è£…: pip install auto-gptq optimum transformers")
        return False

    print("=" * 60)
    print("GPTQ é‡åŒ–é…ç½®")
    print("=" * 60)
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"  é‡åŒ–ä½æ•°: {bits}")
    print(f"  åˆ†ç»„å¤§å°: {group_size}")
    print(f"  æ¿€æ´»æ’åº: {desc_act}")
    print(f"  é˜»å°¼ç³»æ•°: {damp_percent}")
    print(f"  æ ¡å‡†æ•°æ®: {calib_dataset}")
    print(f"  æ ¡å‡†æ ·æœ¬: {calib_samples}")
    print("=" * 60)

    # åŠ è½½ tokenizer
    print("\nğŸ“¦ åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
    )

    # å‡†å¤‡æ ¡å‡†æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ ¡å‡†æ•°æ®...")

    if calib_dataset == "wikitext2":
        from datasets import load_dataset
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
        calib_texts = [text for text in dataset["text"] if len(text) > 100][:calib_samples]
    elif calib_dataset == "c4":
        from datasets import load_dataset
        dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)
        calib_texts = []
        for i, item in enumerate(dataset):
            if len(item["text"]) > 100:
                calib_texts.append(item["text"])
            if len(calib_texts) >= calib_samples:
                break
    else:
        # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        with open(calib_dataset, "r", encoding="utf-8") as f:
            calib_texts = [line.strip() for line in f if len(line.strip()) > 100][:calib_samples]

    # æŒ‰ calib_seq_len æˆªæ–­æ ¡å‡†æ–‡æœ¬ï¼ˆtokenize åæˆªæ–­ï¼‰
    # è¿™ç¡®ä¿æ ¡å‡†æ ·æœ¬é•¿åº¦ä¸€è‡´ï¼Œé¿å…è¿‡é•¿æ ·æœ¬å½±å“é‡åŒ–æ•ˆæœ
    truncated_texts = []
    for text in calib_texts:
        tokens = tokenizer.encode(text, add_special_tokens=False)
        if len(tokens) > calib_seq_len:
            tokens = tokens[:calib_seq_len]
        truncated_texts.append(tokenizer.decode(tokens))
    calib_texts = truncated_texts

    print(f"   å·²åŠ è½½ {len(calib_texts)} æ¡æ ¡å‡†æ ·æœ¬ï¼ˆæˆªæ–­è‡³ {calib_seq_len} tokensï¼‰")

    # é…ç½® GPTQ
    gptq_config = GPTQConfig(
        bits=bits,
        group_size=group_size,
        desc_act=desc_act,
        damp_percent=damp_percent,
        dataset=calib_texts,
        tokenizer=tokenizer,
        use_exllama=use_exllama,
    )

    # åŠ è½½æ¨¡å‹å¹¶é‡åŒ–
    print("\nğŸ”§ åŠ è½½æ¨¡å‹å¹¶é‡åŒ–ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=gptq_config,
        device_map="auto",
        trust_remote_code=True,
    )

    # ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)

    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))

    # è¾“å‡ºç»Ÿè®¡
    total_size = sum(f.stat().st_size for f in output_dir.glob("**/*") if f.is_file())
    print(f"\nâœ… é‡åŒ–å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   æ€»å¤§å°: {total_size / 1024 / 1024 / 1024:.2f} GB")

    return True


def main():
    parser = argparse.ArgumentParser(
        description="GPTQ é‡åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python gptq_quantize.py --model_path ./model --output_path ./model-gptq

  # ä½¿ç”¨ HuggingFace æ¨¡å‹
  python gptq_quantize.py --model_path Qwen/Qwen2.5-7B-Instruct --output_path ./qwen2.5-7b-gptq

  # é«˜ç²¾åº¦æ¨¡å¼ï¼ˆæ›´æ…¢ä½†ç²¾åº¦æ›´é«˜ï¼‰
  python gptq_quantize.py --model_path ./model --output_path ./model-gptq --desc_act true --calib_samples 2048
        """
    )

    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹åï¼‰"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="è¾“å‡ºç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--bits",
        type=int,
        default=4,
        choices=[2, 3, 4, 8],
        help="é‡åŒ–ä½æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    parser.add_argument(
        "--group_size",
        type=int,
        default=128,
        help="é‡åŒ–åˆ†ç»„å¤§å°ï¼ˆé»˜è®¤: 128ï¼‰"
    )
    parser.add_argument(
        "--desc_act",
        type=lambda x: x.lower() == "true",
        default=True,
        help="æ˜¯å¦æŒ‰æ¿€æ´»å€¼æ’åºï¼ˆé»˜è®¤: trueï¼‰"
    )
    parser.add_argument(
        "--damp_percent",
        type=float,
        default=0.1,
        help="é˜»å°¼ç³»æ•°ï¼ˆé»˜è®¤: 0.1ï¼‰"
    )
    parser.add_argument(
        "--calib_dataset",
        type=str,
        default="wikitext2",
        help="æ ¡å‡†æ•°æ®é›†ï¼šwikitext2, c4, æˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: wikitext2ï¼‰"
    )
    parser.add_argument(
        "--calib_samples",
        type=int,
        default=1024,
        help="æ ¡å‡†æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 1024ï¼‰"
    )
    parser.add_argument(
        "--calib_seq_len",
        type=int,
        default=512,
        help="æ ¡å‡†åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 512ï¼‰"
    )
    parser.add_argument(
        "--use_exllama",
        type=lambda x: x.lower() == "true",
        default=True,
        help="æ˜¯å¦ä½¿ç”¨ ExLlama åŠ é€Ÿï¼ˆé»˜è®¤: trueï¼‰"
    )

    args = parser.parse_args()

    success = quantize_gptq(
        model_path=args.model_path,
        output_path=args.output_path,
        bits=args.bits,
        group_size=args.group_size,
        desc_act=args.desc_act,
        damp_percent=args.damp_percent,
        calib_dataset=args.calib_dataset,
        calib_samples=args.calib_samples,
        calib_seq_len=args.calib_seq_len,
        use_exllama=args.use_exllama,
    )

    if not success:
        exit(1)


if __name__ == "__main__":
    main()
