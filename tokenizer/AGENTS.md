# Tokenizer è¯è¡¨æ‰©å……æ¨¡å— - AGENTS.md

## æ¨¡å—æ¦‚è§ˆ

`tokenizer/` æ¨¡å—è´Ÿè´£ **è¯è¡¨æ‰©å……ï¼ˆVocabulary Expansionï¼‰** å·¥ä½œæµï¼Œç›®æ ‡æ˜¯ä¸ºåŸºåº§æ¨¡å‹è¿½åŠ ä¸­æ–‡/é¢†åŸŸè¯è¡¨ï¼Œå‡å°‘ä¸“ä¸šæœ¯è¯­è¢«æ‹†ç¢æˆ– OOVï¼ŒåŒæ—¶ä¿æŒå…¼å®¹æ€§ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š
1. è®°å½•åŸºåº§ tokenizer é…ç½®
2. æ¸…æ´—è¯­æ–™ï¼ˆç™¾ç§‘ + é¢†åŸŸè¯­æ–™ï¼‰
3. è®­ç»ƒè¾…åŠ© tokenizerï¼ˆå¯¹é½åŸºåº§é…ç½®ï¼‰
4. ä»åˆ†è¯å¯¹æ¯”ç­›é€‰æ–°å¢ token
5. å°†æ–° token è¿½åŠ åˆ°åŸºåº§ tokenizer
6. æ‰©å±•æ¨¡å‹ embedding
7. è¯„ä¼°ä¸éªŒè¯

---

## ç›®å½•ç»“æ„

```
tokenizer/
â”œâ”€â”€ aux_tokenizer/       # ğŸ”§ è¾…åŠ© tokenizer è®­ç»ƒï¼ˆæ ¸å¿ƒè„šæœ¬ï¼‰
â”œâ”€â”€ data/                # ğŸ“Š è®­ç»ƒè¯­æ–™æ•°æ®
â”œâ”€â”€ evaluate/            # ğŸ“ˆ è¯„ä¼°è„šæœ¬
â”œâ”€â”€ logs/                # ğŸ“ è¿è¡Œæ—¥å¿—
â”œâ”€â”€ Simple_MLM/          # ğŸ§ª ç®€å• MLM å®éªŒä»£ç 
â”œâ”€â”€ merged_tokenizer/    # ğŸ“¦ [äº§ç‰©] åˆå¹¶åçš„ tokenizer
â”œâ”€â”€ merged_model/        # ğŸ“¦ [äº§ç‰©] æ‰©å±• embedding åçš„æ¨¡å‹
â”œâ”€â”€ merged_model_mlm/    # ğŸ“¦ [äº§ç‰©] MLM é¢„è®­ç»ƒåçš„æ¨¡å‹
â”œâ”€â”€ mlm_tmp/             # ğŸ“¦ [äº§ç‰©] MLM è®­ç»ƒä¸´æ—¶æ–‡ä»¶
â””â”€â”€ æ‰©å……è¯è¡¨å¼€å‘æ–¹æ¡ˆ.md   # ğŸ“– è¯¦ç»†å¼€å‘æ–‡æ¡£
```

---

## æ ¸å¿ƒç›®å½•è¯¦è§£

### `aux_tokenizer/` - è¾…åŠ© tokenizer è®­ç»ƒ

å­˜æ”¾è¾…åŠ© tokenizer çš„è®­ç»ƒè„šæœ¬å’Œäº§ç‰©ï¼Œæ˜¯è¯è¡¨æ‰©å……çš„æ ¸å¿ƒæ¨¡å—ã€‚

| æ–‡ä»¶ | ç”¨é€” |
|-----|------|
| `train_aux_tokenizer.py` | è®­ç»ƒè¾…åŠ© SentencePiece tokenizerï¼ˆBPE/Unigramï¼‰ |
| `extract_tokenizer_config.py` | æå–åŸºåº§ tokenizer é…ç½®ï¼Œç”¨äºå¯¹é½ |
| `select_new_tokens.py` | å¯¹æ¯”åˆ†è¯ç»“æœï¼Œç­›é€‰é«˜é¢‘æ–°å¢ token |
| `extend_base_tokenizer.py` | å°†æ–° token è¿½åŠ åˆ°åŸºåº§ tokenizer |
| `resize_embeddings.py` | æ‰©å±•æ¨¡å‹ embedding å±‚ä»¥é€‚é…æ–°è¯è¡¨ |
| `chinese_spm_20000.model/.vocab` | è®­ç»ƒå¥½çš„è¾…åŠ© tokenizerï¼ˆ20k è¯è¡¨ï¼‰ |
| `new_tokens_list.txt` | ç­›é€‰å‡ºçš„æ–°å¢ token åˆ—è¡¨ |
| `README.md` | ä½¿ç”¨è¯´æ˜ |

**å…¸å‹å·¥ä½œæµ**ï¼š
```bash
# 1. æå–åŸºåº§é…ç½®
uv run python tokenizer/aux_tokenizer/extract_tokenizer_config.py

# 2. è®­ç»ƒè¾…åŠ© tokenizer
uv run python tokenizer/aux_tokenizer/train_aux_tokenizer.py

# 3. ç­›é€‰æ–°å¢ token
uv run python tokenizer/aux_tokenizer/select_new_tokens.py

# 4. æ‰©å……åŸºåº§ tokenizer
uv run python tokenizer/aux_tokenizer/extend_base_tokenizer.py

# 5. æ‰©å±•æ¨¡å‹ embedding
uv run python tokenizer/aux_tokenizer/resize_embeddings.py
```

---

### `data/` - è®­ç»ƒè¯­æ–™

å­˜æ”¾ç”¨äºè®­ç»ƒè¾…åŠ© tokenizer çš„è¯­æ–™æ•°æ®ã€‚

| æ–‡ä»¶/ç›®å½• | ç”¨é€” |
|----------|------|
| `fetch_and_clean.py` | è·å–å¹¶æ¸…æ´—è¯­æ–™çš„è„šæœ¬ |
| `tokenizer_data/` | æ¸…æ´—åçš„è®­ç»ƒè¯­æ–™ï¼ˆtxt æ ¼å¼ï¼‰ |
| `README.md` | æ•°æ®æ¥æºä¸æ¸…æ´—è¯´æ˜ |

**è¯­æ–™è¦æ±‚**ï¼š
- UTF-8 ç¼–ç ï¼Œè¡Œç²’åº¦æ–‡æœ¬
- å»ºè®®ï¼šä¸­æ–‡ç™¾ç§‘ ~8GB + é¢†åŸŸè¯­æ–™
- å·²æ¸…æ´—ï¼šå» HTML/Markdownã€å»é‡ã€è¿‡æ»¤è¿‡çŸ­/è¿‡é•¿

---

### `evaluate/` - è¯„ä¼°è„šæœ¬

ç”¨äºè¯„ä¼°æ‰©å……å tokenizer çš„è´¨é‡ã€‚

| æ–‡ä»¶ | ç”¨é€” |
|-----|------|
| `eval_tokenizer.py` | åˆ†è¯å¯¹æ¯”ï¼šOOV ç‡ã€token æ•°ã€è¦†ç›–ç‡ |
| `eval_mlm_loss.py` | è¯„ä¼° MLM æ¨¡å‹çš„å›°æƒ‘åº¦/Loss |
| `README.md` | è¯„ä¼°æŒ‡å— |

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- **OOV ç‡**ï¼šæœªçŸ¥è¯æ¯”ä¾‹ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **å¹³å‡ token æ•°**ï¼šåŒä¸€æ–‡æœ¬çš„ token æ•°å˜åŒ–
- **å›°æƒ‘åº¦**ï¼šè¯­è¨€æ¨¡å‹è¯„ä¼°ï¼ˆéœ€è¦æ¨¡å‹è®­ç»ƒåï¼‰

---

### `logs/` - è¿è¡Œæ—¥å¿—

å­˜æ”¾å„é˜¶æ®µçš„è¿è¡Œæ—¥å¿—ï¼Œç”¨äºè¿½æº¯å’Œè°ƒè¯•ã€‚

- `train_spm.log` - è¾…åŠ© tokenizer è®­ç»ƒæ—¥å¿—
- `merge.log` - tokenizer åˆå¹¶æ—¥å¿—
- `eval_report.md` - è¯„ä¼°æŠ¥å‘Š

---

### `Simple_MLM/` - MLM å®éªŒä»£ç 

ç®€å•çš„ Masked Language Model å®éªŒä»£ç ï¼Œç”¨äºéªŒè¯æ‰©å……åè¯è¡¨çš„æœ‰æ•ˆæ€§ã€‚

---

## äº§ç‰©ç›®å½•ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰

ä»¥ä¸‹ç›®å½•æ˜¯è„šæœ¬è¿è¡Œåçš„äº§ç‰©ï¼Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºæˆ–ä¿®æ”¹ï¼š

| ç›®å½• | è¯´æ˜ |
|-----|------|
| `merged_tokenizer/` | åˆå¹¶æ–° token åçš„ tokenizer æ–‡ä»¶ï¼ˆå¯ç›´æ¥ç”¨äºè®­ç»ƒï¼‰ |
| `merged_model/` | æ‰©å±• embedding åçš„æ¨¡å‹ï¼ˆå¯ç”¨äºåç»­ CPT/SFTï¼‰ |
| `merged_model_mlm/` | ç»è¿‡ MLM é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆæ–° token å·²æœ‰è¯­ä¹‰ï¼‰ |
| `mlm_tmp/` | MLM è®­ç»ƒè¿‡ç¨‹çš„ä¸´æ—¶æ–‡ä»¶å’Œ checkpoint |

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ‰©å……åçš„ tokenizer
tokenizer = AutoTokenizer.from_pretrained("tokenizer/merged_tokenizer")

# åŠ è½½æ‰©å±• embedding åçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("tokenizer/merged_model")
```

---

## é…ç½®æ–‡ä»¶

è¯è¡¨æ‰©å……çš„é…ç½®ä½äº `config/tokenizer_config/config.yaml`ï¼š

```yaml
# åŸºåº§ tokenizer
base_tokenizer: bert-base-chinese

# è¾…åŠ© tokenizer è®­ç»ƒå‚æ•°
aux_tokenizer:
  vocab_size: 20000        # æ–°å¢è¯è¡¨å¤§å°
  model_type: bpe          # BPE æˆ– Unigram
  character_coverage: 0.9995
  split_digits: true
  byte_fallback: true

# è¾“å‡ºè·¯å¾„
output:
  tokenizer_path: tokenizer/merged_tokenizer
  model_path: tokenizer/merged_model
```

---

## æ³¨æ„äº‹é¡¹

âš ï¸ **åªè¿½åŠ ä¸åˆ é™¤**ï¼šæ–° token åªèƒ½è¿½åŠ åˆ°è¯è¡¨æœ«å°¾ï¼Œä¸èƒ½åˆ é™¤æˆ–é‡æ’åŸºåº§ token

âš ï¸ **éœ€è¦è®­ç»ƒ**ï¼šæ–°å¢ token çš„ embedding æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œéœ€è¦ CPT/SFT æ‰èƒ½è·å¾—è¯­ä¹‰

âš ï¸ **ç‰¹æ®Š token é¡ºåº**ï¼šç¡®ä¿ç‰¹æ®Š tokenï¼ˆå¦‚ `<|im_start|>`ï¼‰é¡ºåºä¸åŸºåº§ä¸€è‡´

âš ï¸ **å¤‡ä»½**ï¼šä¿ç•™æ—§ tokenizer/æ¨¡å‹å¤‡ä»½ï¼Œä¾¿äºå›æ»š

---

## ç›¸å…³æ–‡æ¡£

- [æ‰©å……è¯è¡¨å¼€å‘æ–¹æ¡ˆ](./æ‰©å……è¯è¡¨å¼€å‘æ–¹æ¡ˆ.md) - å®Œæ•´å¼€å‘æµç¨‹ä¸ç»†èŠ‚
- [aux_tokenizer/README.md](./aux_tokenizer/README.md) - è®­ç»ƒè„šæœ¬ä½¿ç”¨è¯´æ˜
- [evaluate/README.md](./evaluate/README.md) - è¯„ä¼°æ–¹æ³•è¯´æ˜
- [data/README.md](./data/README.md) - æ•°æ®å‡†å¤‡è¯´æ˜
