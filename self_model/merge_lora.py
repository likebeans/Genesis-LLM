#!/usr/bin/env python3
"""
LoRA æƒé‡åˆå¹¶è„šæœ¬

å°† LoRA é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºåº§æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´çš„å¯éƒ¨ç½²æ¨¡å‹ã€‚

ç”¨æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•
    uv run python self_model/merge_lora.py \
        --base_model Qwen/Qwen2.5-0.5B-Instruct \
        --lora_path self_model/checkpoints/finetune \
        --output_path self_model/checkpoints/finetune_merged

    # ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹
    uv run python self_model/merge_lora.py \
        --base_model /path/to/local/model \
        --lora_path self_model/checkpoints/finetune \
        --output_path self_model/checkpoints/finetune_merged

    # ä¿å­˜ä¸º safetensors æ ¼å¼
    uv run python self_model/merge_lora.py \
        --base_model Qwen/Qwen2.5-0.5B-Instruct \
        --lora_path self_model/checkpoints/finetune \
        --output_path self_model/checkpoints/finetune_merged \
        --safe_serialization
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


def merge_lora(
    base_model_path: str,
    lora_path: str,
    output_path: str,
    device_map: str = "auto",
    torch_dtype: str = "auto",
    safe_serialization: bool = True,
    push_to_hub: bool = False,
    hub_repo_id: str | None = None,
):
    """
    åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹

    Args:
        base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹åï¼‰
        lora_path: LoRA é€‚é…å™¨è·¯å¾„
        output_path: åˆå¹¶åæ¨¡å‹çš„è¾“å‡ºè·¯å¾„
        device_map: è®¾å¤‡æ˜ å°„ç­–ç•¥
        torch_dtype: æ¨¡å‹ç²¾åº¦
        safe_serialization: æ˜¯å¦ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜
        push_to_hub: æ˜¯å¦æ¨é€åˆ° HuggingFace Hub
        hub_repo_id: HuggingFace Hub ä»“åº“ ID
    """

    print("=" * 60)
    print("LoRA æƒé‡åˆå¹¶")
    print("=" * 60)
    print(f"  åŸºåº§æ¨¡å‹: {base_model_path}")
    print(f"  LoRA è·¯å¾„: {lora_path}")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")
    print("=" * 60)

    # ç¡®å®šæ•°æ®ç±»å‹
    if torch_dtype == "auto":
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    elif torch_dtype == "float16":
        dtype = torch.float16
    elif torch_dtype == "bfloat16":
        dtype = torch.bfloat16
    else:
        dtype = torch.float32

    print(f"\nğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map=device_map,
        trust_remote_code=True,
    )

    print(f"\nğŸ”§ åŠ è½½ LoRA é€‚é…å™¨: {lora_path}")
    model = PeftModel.from_pretrained(
        base_model,
        lora_path,
        torch_dtype=dtype,
    )

    # æ‰“å° LoRA é…ç½®ä¿¡æ¯
    lora_config_path = Path(lora_path) / "adapter_config.json"
    if lora_config_path.exists():
        with open(lora_config_path) as f:
            lora_config = json.load(f)
        print(f"\nğŸ“‹ LoRA é…ç½®:")
        print(f"   - r (rank): {lora_config.get('r', 'N/A')}")
        print(f"   - lora_alpha: {lora_config.get('lora_alpha', 'N/A')}")
        print(f"   - target_modules: {lora_config.get('target_modules', 'N/A')}")

    print("\nğŸ”€ åˆå¹¶æƒé‡...")
    merged_model = model.merge_and_unload()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    merged_model.save_pretrained(
        output_path,
        safe_serialization=safe_serialization,
    )

    # åŠ è½½å¹¶ä¿å­˜ tokenizer
    print("\nğŸ’¾ ä¿å­˜ tokenizer...")
    # ä¼˜å…ˆä» LoRA è·¯å¾„åŠ è½½ tokenizerï¼ˆå¯èƒ½æœ‰ä¿®æ”¹ï¼‰
    try:
        tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)
    except Exception:
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

    tokenizer.save_pretrained(output_path)

    # è®¡ç®—æ¨¡å‹å¤§å°
    total_size = sum(
        f.stat().st_size for f in output_dir.glob("**/*") if f.is_file()
    )

    print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"   æ€»å¤§å°: {total_size / 1024 / 1024 / 1024:.2f} GB")

    # æ¨é€åˆ° Hub
    if push_to_hub and hub_repo_id:
        print(f"\nğŸš€ æ¨é€åˆ° HuggingFace Hub: {hub_repo_id}")
        merged_model.push_to_hub(hub_repo_id)
        tokenizer.push_to_hub(hub_repo_id)
        print("   æ¨é€å®Œæˆï¼")

    return merged_model, tokenizer


def main():
    parser = argparse.ArgumentParser(
        description="LoRA æƒé‡åˆå¹¶å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆå¹¶ LoRA åˆ°åŸºåº§æ¨¡å‹
  python merge_lora.py \\
    --base_model Qwen/Qwen2.5-0.5B-Instruct \\
    --lora_path self_model/checkpoints/finetune \\
    --output_path self_model/checkpoints/finetune_merged

  # ä½¿ç”¨ float32 ç²¾åº¦
  python merge_lora.py \\
    --base_model Qwen/Qwen2.5-0.5B-Instruct \\
    --lora_path self_model/checkpoints/finetune \\
    --output_path self_model/checkpoints/finetune_merged \\
    --torch_dtype float32
        """
    )

    parser.add_argument(
        "--base_model",
        type=str,
        required=True,
        help="åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹åï¼‰"
    )
    parser.add_argument(
        "--lora_path",
        type=str,
        required=True,
        help="LoRA é€‚é…å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="åˆå¹¶åæ¨¡å‹çš„è¾“å‡ºè·¯å¾„"
    )
    parser.add_argument(
        "--device_map",
        type=str,
        default="auto",
        help="è®¾å¤‡æ˜ å°„ç­–ç•¥ï¼ˆé»˜è®¤: autoï¼‰"
    )
    parser.add_argument(
        "--torch_dtype",
        type=str,
        default="auto",
        choices=["auto", "float16", "bfloat16", "float32"],
        help="æ¨¡å‹ç²¾åº¦ï¼ˆé»˜è®¤: autoï¼‰"
    )
    parser.add_argument(
        "--safe_serialization",
        action="store_true",
        default=True,
        help="ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜ï¼ˆé»˜è®¤: Trueï¼‰"
    )
    parser.add_argument(
        "--no_safe_serialization",
        action="store_true",
        help="ä½¿ç”¨ pytorch bin æ ¼å¼ä¿å­˜"
    )
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="æ¨é€åˆ° HuggingFace Hub"
    )
    parser.add_argument(
        "--hub_repo_id",
        type=str,
        default=None,
        help="HuggingFace Hub ä»“åº“ ID"
    )

    args = parser.parse_args()

    # å¤„ç† safe_serialization
    safe_serialization = not args.no_safe_serialization

    merge_lora(
        base_model_path=args.base_model,
        lora_path=args.lora_path,
        output_path=args.output_path,
        device_map=args.device_map,
        torch_dtype=args.torch_dtype,
        safe_serialization=safe_serialization,
        push_to_hub=args.push_to_hub,
        hub_repo_id=args.hub_repo_id,
    )


if __name__ == "__main__":
    main()
